usage: run.py [-h] --output_dir OUTPUT_DIR
              [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]
              [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]
              [--do_predict [DO_PREDICT]]
              [--evaluation_strategy {no,steps,epoch}]
              [--prediction_loss_only [PREDICTION_LOSS_ONLY]]
              [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]
              [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]
              [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
              [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
              [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
              [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]
              [--eval_delay EVAL_DELAY] [--learning_rate LEARNING_RATE]
              [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]
              [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]
              [--max_grad_norm MAX_GRAD_NORM]
              [--num_train_epochs NUM_TRAIN_EPOCHS] [--max_steps MAX_STEPS]
              [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
              [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]
              [--log_level {debug,info,warning,error,critical,passive}]
              [--log_level_replica {debug,info,warning,error,critical,passive}]
              [--log_on_each_node [LOG_ON_EACH_NODE]] [--no_log_on_each_node]
              [--logging_dir LOGGING_DIR]
              [--logging_strategy {no,steps,epoch}]
              [--logging_first_step [LOGGING_FIRST_STEP]]
              [--logging_steps LOGGING_STEPS]
              [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]
              [--no_logging_nan_inf_filter] [--save_strategy {no,steps,epoch}]
              [--save_steps SAVE_STEPS] [--save_total_limit SAVE_TOTAL_LIMIT]
              [--save_on_each_node [SAVE_ON_EACH_NODE]] [--no_cuda [NO_CUDA]]
              [--seed SEED] [--data_seed DATA_SEED]
              [--jit_mode_eval [JIT_MODE_EVAL]] [--use_ipex [USE_IPEX]]
              [--bf16 [BF16]] [--fp16 [FP16]]
              [--fp16_opt_level FP16_OPT_LEVEL]
              [--half_precision_backend {auto,cuda_amp,apex,cpu_amp}]
              [--bf16_full_eval [BF16_FULL_EVAL]]
              [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]
              [--local_rank LOCAL_RANK] [--xpu_backend {mpi,ccl}]
              [--tpu_num_cores TPU_NUM_CORES]
              [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug DEBUG]
              [--dataloader_drop_last [DATALOADER_DROP_LAST]]
              [--eval_steps EVAL_STEPS]
              [--dataloader_num_workers DATALOADER_NUM_WORKERS]
              [--past_index PAST_INDEX] [--run_name RUN_NAME]
              [--disable_tqdm DISABLE_TQDM]
              [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]
              [--no_remove_unused_columns]
              [--label_names LABEL_NAMES [LABEL_NAMES ...]]
              [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]
              [--metric_for_best_model METRIC_FOR_BEST_MODEL]
              [--greater_is_better GREATER_IS_BETTER]
              [--ignore_data_skip [IGNORE_DATA_SKIP]]
              [--sharded_ddp SHARDED_DDP] [--fsdp FSDP]
              [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]
              [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]
              [--deepspeed DEEPSPEED]
              [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]
              [--optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_bnb_8bit,sgd,adagrad}]
              [--adafactor [ADAFACTOR]] [--group_by_length [GROUP_BY_LENGTH]]
              [--length_column_name LENGTH_COLUMN_NAME]
              [--report_to REPORT_TO [REPORT_TO ...]]
              [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]
              [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]
              [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]
              [--no_dataloader_pin_memory]
              [--skip_memory_metrics [SKIP_MEMORY_METRICS]]
              [--no_skip_memory_metrics]
              [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]
              [--push_to_hub [PUSH_TO_HUB]]
              [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
              [--hub_model_id HUB_MODEL_ID]
              [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]
              [--hub_token HUB_TOKEN] [--hub_private_repo [HUB_PRIVATE_REPO]]
              [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]
              [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]
              [--fp16_backend {auto,cuda_amp,apex,cpu_amp}]
              [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]
              [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]
              [--push_to_hub_token PUSH_TO_HUB_TOKEN]
              [--mp_parameters MP_PARAMETERS]
              [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]
              [--full_determinism [FULL_DETERMINISM]]
              [--torchdynamo {eager,nvfuser,fx2trt,fx2trt-fp16}]
              [--ray_scope RAY_SCOPE] [--model_name MODEL_NAME]
              [--model_path MODEL_PATH] [--max_length MAX_LENGTH]
              [--id_max_length ID_MAX_LENGTH]
              [--remove_prompt [REMOVE_PROMPT]] [--train_file TRAIN_FILE]
              [--valid_file VALID_FILE] [--task TASK] [--top_k TOP_K]
              [--num_return_sequences NUM_RETURN_SEQUENCES]
              [--q_max_length Q_MAX_LENGTH] [--table_name TABLE_NAME]
              [--db_name DB_NAME] [--train_size TRAIN_SIZE]
              [--validate_size VALIDATE_SIZE] [--test_size TEST_SIZE]
              [--query_type QUERY_TYPE] [--split_by SPLIT_BY]
run.py: error: argument --save_total_limit: invalid int value: 'None'
E1220 16:21:59.628333 141503 .venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 2) local_rank: 0 (pid: 141523) of binary: /gpfs/work5/0/prjs1828/DSI-QG/.venv/bin/python
Traceback (most recent call last):
  File "/gpfs/work5/0/prjs1828/DSI-QG/.venv/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/gpfs/work5/0/prjs1828/DSI-QG/.venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/gpfs/work5/0/prjs1828/DSI-QG/.venv/lib/python3.9/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/gpfs/work5/0/prjs1828/DSI-QG/.venv/lib/python3.9/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/gpfs/work5/0/prjs1828/DSI-QG/.venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/gpfs/work5/0/prjs1828/DSI-QG/.venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-20_16:21:59
  host      : tcn376.local.snellius.surf.nl
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 141523)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
