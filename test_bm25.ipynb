{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3a8df",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from typing import List, Dict\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from CE.utils.database import save_result\n",
    "from CE.utils.test.general import BaseMetricCalculator\n",
    "from CE.utils.database import load_db\n",
    "import argparse\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DF = pd.DataFrame(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f622a7c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class BM25EmailSearchEvaluator(BaseMetricCalculator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_file,\n",
    "        table_name,\n",
    "        corpus_dir=\"corpus_data\",\n",
    "        index_dir=\"indexes/enron_index\",\n",
    "        threads=4,\n",
    "    ):\n",
    "        self.table_name = table_name\n",
    "        self.input_file = input_file\n",
    "        self.corpus_dir = corpus_dir\n",
    "        self.index_dir = index_dir\n",
    "        self.threads = threads\n",
    "\n",
    "        self.mid_to_textid = {}\n",
    "        self.queries = []\n",
    "        # self.queries_textrank = []\n",
    "        # self.queries_d2q = []\n",
    "\n",
    "        # Phase 1 storage: Raw Ranks\n",
    "        self.execution_results = []\n",
    "        self.final_metrics = []\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if os.path.exists(self.corpus_dir):\n",
    "            shutil.rmtree(self.corpus_dir)\n",
    "        os.makedirs(self.corpus_dir)\n",
    "\n",
    "        with open(self.input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "            for line in fin:\n",
    "                data = json.loads(line)\n",
    "                text_id, body = (\n",
    "                    data.get(\"text_id\"),\n",
    "                    data.get(\"body_clean_and_subject\", \"\"),\n",
    "                )\n",
    "\n",
    "                # fout.write(json.dumps({\"id\": mid, \"contents\": body}) + \"\\n\")\n",
    "                # self.mid_to_textid[mid] = text_id\n",
    "                if data.get(\"text\"):\n",
    "                    self.queries.append(\n",
    "                        {\"target_text_id\": text_id, \"query\": data[\"text\"]}\n",
    "                    )\n",
    "\n",
    "                # if data.get(\"text_rank_query\"):\n",
    "                #     self.queries_textrank.append(\n",
    "                #         {\"target_text_id\": text_id, \"query\": data[\"text_rank_query\"]}\n",
    "                #     )\n",
    "                # if data.get(\"doctoquery\"):\n",
    "                #     self.queries_d2q.append(\n",
    "                #         {\"target_text_id\": text_id, \"query\": data[\"doctoquery\"]}\n",
    "                # )\n",
    "        with open(\n",
    "            os.path.join(self.corpus_dir, \"docs.jsonl\"), \"w\", encoding=\"utf-8\"\n",
    "        ) as fout:\n",
    "\n",
    "            if self.table_name == \"test_table\":\n",
    "                # df = \n",
    "                pass\n",
    "            else:\n",
    "                df = load_db(self.table_name)\n",
    "            for _, row in df.iterrows():\n",
    "                mid = str(row[\"mid\"])\n",
    "                body = row[\"body_clean_and_subject\"]\n",
    "                fout.write(json.dumps({\"id\": mid, \"contents\": body}) + \"\\n\")\n",
    "                self.mid_to_textid[mid] = row[\"elaborative_description\"]\n",
    "\n",
    "    def build_index(self):\n",
    "        if os.path.exists(self.index_dir):\n",
    "            shutil.rmtree(self.index_dir)\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pyserini.index.lucene\",\n",
    "            \"--collection\",\n",
    "            \"JsonCollection\",\n",
    "            \"--input\",\n",
    "            self.corpus_dir,\n",
    "            \"--index\",\n",
    "            self.index_dir,\n",
    "            \"--generator\",\n",
    "            \"DefaultLuceneDocumentGenerator\",\n",
    "            \"--threads\",\n",
    "            str(self.threads),\n",
    "            \"--storePositions\",\n",
    "            \"--storeDocvectors\",\n",
    "            \"--storeRaw\",\n",
    "        ]\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "    def run_retrieval_phase(self, k1=0.9, b=0.4):\n",
    "        \"\"\"Phase 1: Run all datapoints and extract ranks.\"\"\"\n",
    "        searcher = LuceneSearcher(self.index_dir)\n",
    "        searcher.set_bm25(k1=k1, b=b)\n",
    "\n",
    "        ranks = []\n",
    "        for q in self.queries:\n",
    "            hits = searcher.search(q[\"query\"], k=20)\n",
    "            rank = float(\"inf\")\n",
    "            for i, hit in enumerate(hits):\n",
    "                if self.mid_to_textid.get(hit.docid) == q[\"target_text_id\"]:\n",
    "                    rank = i + 1\n",
    "                    break\n",
    "            ranks.append(rank)\n",
    "        self.execution_results = ranks\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        \"\"\"Phase 2: Calculate metrics from stored ranks.\"\"\"\n",
    "        ranks = self.execution_results\n",
    "\n",
    "        mrr3 = self.calculate_mrr(ranks, 3)\n",
    "        mrr20 = self.calculate_mrr(ranks, 20)\n",
    "        hits1 = self.calculate_hits(ranks, 1)\n",
    "        hits10 = self.calculate_hits(ranks, 10)\n",
    "\n",
    "        self.final_metrics = [f\"{value:.4f}\" for value in [mrr3, mrr20, hits1, hits10]]\n",
    "        print(f\"\\nResults for: MRR@3: {mrr3:.4f}, Hits@1: {hits1:.4f}\")\n",
    "\n",
    "    def save_results(self, size: str, experiment_type: str, version: str = \"v1.0\"):\n",
    "        data = [\"BM25-base\", size, experiment_type, version] + self.final_metrics\n",
    "        save_result(tuple(data))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091335fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"test_table\"\n",
    "\n",
    "evaluator = BM25EmailSearchEvaluator(\n",
    "    input_file=f\"data/test.{table_name}.docTquery\", table_name=table_name\n",
    ")\n",
    "evaluator.prepare_data()\n",
    "evaluator.build_index()\n",
    "evaluator.run_retrieval_phase()\n",
    "evaluator.compute_metrics()\n",
    "evaluator.save_results(\"10k\", \"no_thread\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
